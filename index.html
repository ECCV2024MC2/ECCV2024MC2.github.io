<script src="https://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
    google.load("jquery", "1.3.2");
</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>


<script src="index.js" type="text/javascript"></script>

<link href="style.css" rel="stylesheet" type="text/css" />
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>
<style>
    @import url('https://fonts.googleapis.com/css2?family=Varela+Round&display=swap');
</style>

<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif,
            "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Changa One:400,400italic", "Varela Round:400";
        font-weight: 300;
        font-size: 24px;
        margin-left: auto;
        margin-right: auto;
        width: 1600px;
        color: #666;
    }

    h1 {
        color: #000;
        font-size: 40px;
        font-weight: 300;
        width: 1572px;
    }

    h2 {
        color: #444;
        font-size: 35px;
        font-weight: 300;
        width: 1572px;
    }

    h3 {
        color: #444;
        font-size: 30px;
        font-weight: 300;
        width: 1572px;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 20px;
    }


    table td {
        font-size: 24px;
        line-height: 1.15em;
    }

    .layered-paper-big {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35),
            /* The third layer shadow */
            15px 15px 0 0px #fff,
            /* The fourth layer */
            15px 15px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fourth layer shadow */
            20px 20px 0 0px #fff,
            /* The fifth layer */
            20px 20px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fifth layer shadow */
            25px 25px 0 0px #fff,
            /* The fifth layer */
            25px 25px 1px 1px rgba(0, 0, 0, 0.35);
        /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }

    .paper-big {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35);
        /* The top layer shadow */

        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35);
        /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    .material-icons {
        vertical-align: -6px;
    }

    .paper-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        /* background-color: #68a4fd;
		color: #ecf0f1 !important; */
        /* background-color: #ffffff; */
        color: #3b66a7 !important;
        font-size: 20px;
        width: 100px;
        font-weight: 600;
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
    }

    .paper-btn:hover {
        opacity: 0.85;
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }

    .venue {
        color: #1367a7;
    }
</style>

<html>

<head>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <title style="">MC2: Multi-view Consistent Depth Estimation via Coordinated Image-based Neural Rendering</title>
    <meta property="og:image" content="resources/overview.png" />
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <!-- <meta property="og:video" content="resources/video/teaser/teaser.mp4"> -->
    <meta property="og:title" content="Multi-view Consistent Depth Estimation via Coordinated Image-based Neural Rendering" />
    <meta property="og:description"
        content="Multi-view consistent depth estimation w.r.t the 3D scene where given a stream of images exist" />

    <!-- Get from Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src=""></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-75863369-6');
    </script>
</head>

<body>
    <br>
    <br>
    <br>
    <center>
        <span style="font-size:40px; color:#000;" width=1800px>
            <b>
                MC<sup>2</sup>: Multi-view Consistent Depth Estimation via Coordinated Image-based Neural Rendering
            </b>
        </span>


        <br>

    </center>


	<br><br><br><br>
    <center>
        <h3><b>Note:</b> If some visuals are not displaying correctly, please try refreshing the page
            <br><br><br>
            Best viewed on a monitor and with a Chrome browser
        </h3>

        <br><br>
        <h2><b>TL; DR: Multi-view consistent depth estimation w.r.t the 3D scene where given a stream of images exist</b></h2>
    </center>

	<br><br>
	<hr>
	<br><br>

    <div class="docs-section" id="abstract">

		<center>
            <h1 class="navbar-heading"> <b>Abstract</b></h1>
        </center>

        <table align=center width=1350px>
            <tr>
                <td style="text-align:justify;">
					We are interested in achieving spatially accurate and temporally consistent depth estimates only from a stream of 2D RGB images.
					Despite the success of recent depth estimation methods, 
					we find that this is still difficult since existing approaches often estimate depth only from 2D information 
					and overlook how the scene exists in 3D space.
					To tackle the issue, we propose 
					<em>Multi-view Consistent depth estimation via Coordinated image-based neural rendering</em> 
					<!-- Multi-view Consistent depth estimation via Coordinated image-based neural rendering -->
					(MC<sup>2</sup>) which casts the depth estimation as a feature matching problem in 3D space, 
					thereby constructing and aligning scene features directly in 3D space from 2D images.
					First, we introduce a rescaling technique that minimizes the ambiguity of the depth estimation obtained independently from each 2D image.
					Using 2D images and corresponding rescaled depths, 
					we extract the context representation with our new transformer architecture consisting of three-way factorized attention.
					Moreover, to ensure alignment with 3D structures without explicit geometry modeling, 
					we propose an ordinal volume rendering that respects the nature of 3D spaces.
					We perform extensive comparisons on casually captured scenes from various real-world datasets 
					and significantly outperform previous work in depth estimation from a stream of 2D RGB images.
					Results highlight our method as a comprehensive framework 
					that not only improves the accuracy of monocular estimates 
					but also bridges the gap to multi-view consistent depth estimation 
					that respects the 3D worlds existing in given images.
                </td>
            </tr>
        </table>

            <br>
            <hr>

        <center>

            <div class="video-comp-container">
                <img id="overview" src="resources/1600x900_tmp.gif"  width="1600">
            </div>

            <table align=center width=1350px>
                <tr>
                    <td style="text-align:justify;">
                        MC<sup>2</sup> synthesizes a depth map and corresponding RGB image at <em>arbitrary camera angles</em> using a stream of 2D RGB images as context features. 
                        Such an accurate depth prediction is enabled by three main components. 
                        First, MC<sup>2</sup> rescales each depth estimate of the context views obtained from the monocular depth estimation network to make them exist in the 3D world 
                        in which the scene exists and ensure consistency between individually obtained estimates, 
                        thus enabling geometric priors during feature matching in the next phase. 
                        Then, these rescaled depth features with image features are fed into our three-way factorized transformers 
                        that decompose the features along view, ray, and pixel to efficiently find the correspondence between the context features.
                        Finally, MC<sup>2</sup> renders a depth map without explicit proxy geometry by taking into account the sequential nature of a ray. 
                        By doing so, MC<sup>2</sup> successfully imagines spatially accurate and temporally consistent depth maps that respect the 3D world in which the given images reside.
                    
                    </td>
                </tr>
            </table>


        </center>

    </div>

    <br><br>


    <div class="docs-section" id="mde">

		<center>
            <h1 class="navbar-heading"> <b>Metric Depth Estimation for Mobile Phone Captures</b></h1>
        </center>

        <table align=center width=1350px>
            <tr>
                <td style="text-align:justify;">
					Illustration of depth estimates with corresponding synthesized RGB images on mochi-high-five sequence on the iPhone dataset.
                    The color bar on the right is in meters (m). 
                    MC<sup>2</sup> depth consistently over the video, 
                    while accurately estimating metric depth. 
                    ZoeDepth, on the other hand, overestimates depth,
                    with its estimates fluctuating over time, as shown by the different colors.
                </td>
            </tr>
        </table>

        <br><br>

        <div class="video-container">
            <video id="more-video" autobuffer="" muted="" autoplay="" webkit-playsinline="" playsinline="" loop=""
                src="resources/t23d.mp4">
                <source id="more-video" src="resources/t23d.mp4" type="video/mp4">
            </video>
        </div>

    </div>

	<br><br>

    <div class="docs-section" id="sde">

		<center>
            <h1 class="navbar-heading"> <b>Scaled Depth Estimation for SfM Reconstruction</b></h1>
        </center>

        <table align=center width=1350px>
            <tr>
                <td style="text-align:justify;">
                    Illustration of depth estimates and the corresponding synthesized RGB images. 
                    MC<sup>2</sup>renders a spatially accurate depth map while capturing thin structures, 
                    e.g., wire and radiator (upper) and chair legs (bottom), 
                    which is often missing even with Ground Truth obtained with lidar sensors. 
                    In addition, MC<sup>2</sup> synthesizes deblurred color images for a target view 
                    by aggregating features from context views.
                </td>
            </tr>
        </table>

        <br><br>

        <div class="video-container">
            <video id="more-video" autobuffer="" muted="" autoplay="" webkit-playsinline="" playsinline="" loop=""
                src="resources/t23d.mp4">
                <source id="more-video" src="resources/t23d.mp4" type="video/mp4">
            </video>
        </div>

    </div>

    <br><br>

    <div class="docs-section" id="motivation">

		<center>
            <h1 class="navbar-heading"> <b>Plausible Novel Views Do Not Always Guarantee Accurate Depth Estimation</b></h1>
        </center>

        <table align=center width=1350px>
            <tr>
                <td style="text-align:justify;">
                    Neural scene renderings have shown great potential in parameterizing complex 3D scenes as a neural network 
                    by mapping 5D coordinates to RGB values and densities using 1) NeRF or 2) image-based view synthesis. 
                    
                    <br><br>

                    While the synthesized views are plausible at unseen views and seemingly satisfactory, 
                    they often suffer from inaccurate correspondence modeling with ground-truth 3D scenes. 
                </td>
            </tr>
        </table>

        <br><br>

        <div class="video-container">
            <video id="more-video" autobuffer="" muted="" autoplay="" webkit-playsinline="" playsinline="" loop=""
                src="resources/t23d.mp4">
                <source id="more-video" src="resources/t23d.mp4" type="video/mp4">
            </video>
        </div>

        <br>
        <hr>
        <br>

        <table align=center width=1350px>
            <tr>
                <td style="text-align:justify;">

                    Moreover, we find that existing image-based neural rendering approaches can easily prioritize 
                    the simpler task of blending colors using features obtained from contextual views 
                    over the more complex task of establishing correspondence between sampled contextual features along rays in 3D space.
                    This occurs because image-based neural rendering operated in 2D-pixel space often lacks a comprehensive understanding of the 3D scene, 
                    leading to a preference for color blending over accurate correspondence matching of 3D points, 
                    where depth estimates aligned with camera poses suggest that inferred geometry could be interpreted as correspondence matching points. 
                    This challenge is compounded when the model relies solely on RGB images, 
                    as RGB matching can occur even when each projected pixel location in the context views represents different objects, in the absence of geometric priors.
                
                </td>
            </tr>
        </table>

        <br><br>

        <div class="video-container">
            <video id="more-video" autobuffer="" muted="" autoplay="" webkit-playsinline="" playsinline="" loop=""
                src="resources/t23d.mp4">
                <source id="more-video" src="resources/t23d.mp4" type="video/mp4">
            </video>
        </div>

    </div>

    <br><br>
    <!-- <hr> -->
	<br><br>
</body>

</html>