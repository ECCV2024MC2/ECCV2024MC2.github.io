<script src="https://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
    google.load("jquery", "1.3.2");
</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>


<script src="index.js" type="text/javascript"></script>

<link href="style.css" rel="stylesheet" type="text/css" />
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>
<style>
    @import url('https://fonts.googleapis.com/css2?family=Varela+Round&display=swap');
</style>

<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif,
            "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Changa One:400,400italic", "Varela Round:400";
        font-weight: 300;
        font-size: 24px;
        margin-left: auto;
        margin-right: auto;
        width: 1600px;
        color: #666;
    }

    h1 {
        color: #000;
        font-size: 40px;
        font-weight: 300;
        width: 1572px;
    }

    h2 {
        color: #444;
        font-size: 35px;
        font-weight: 300;
        width: 1572px;
    }

    h3 {
        color: #444;
        font-size: 30px;
        font-weight: 300;
        width: 1572px;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 20px;
    }


    table td {
        font-size: 24px;
        line-height: 1.15em;
    }

    .layered-paper-big {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35),
            /* The third layer shadow */
            15px 15px 0 0px #fff,
            /* The fourth layer */
            15px 15px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fourth layer shadow */
            20px 20px 0 0px #fff,
            /* The fifth layer */
            20px 20px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fifth layer shadow */
            25px 25px 0 0px #fff,
            /* The fifth layer */
            25px 25px 1px 1px rgba(0, 0, 0, 0.35);
        /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }

    .paper-big {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35);
        /* The top layer shadow */

        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35);
        /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    .material-icons {
        vertical-align: -6px;
    }

    .paper-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        /* background-color: #68a4fd;
		color: #ecf0f1 !important; */
        /* background-color: #ffffff; */
        color: #3b66a7 !important;
        font-size: 20px;
        width: 100px;
        font-weight: 600;
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
    }

    .paper-btn:hover {
        opacity: 0.85;
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }

    .venue {
        color: #1367a7;
    }
</style>

<html>

<head>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <title style="">MC<sup>2</sup>: Multi-view Consistent Depth Estimation via Coordinated Image-based Neural Rendering</title>
    <meta property="og:image" content="resources/overview.png" />
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <!-- <meta property="og:video" content="resources/video/teaser/teaser.mp4"> -->
    <meta property="og:title" content="Multi-view Consistent Depth Estimation via Coordinated Image-based Neural Rendering" />
    <meta property="og:description"
        content="Multi-view consistent depth estimation w.r.t the 3D scene where given a stream of images exist" />

    <!-- Get from Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src=""></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-75863369-6');
    </script>
</head>

<body>
    <br>
    <br>
    <br>
    <center>
        <span style="font-size:40px; color:#000;" width=1800px>
            <b>
                MC<sup>2</sup>: Multi-view Consistent Depth Estimation via Coordinated Image-based Neural Rendering
            </b>
        </span>


        <br>

    </center>


	<br><br><br><br>
    <center>
        <h3><b>Note:</b> If some visuals are not displaying correctly, please try refreshing the page
            <br><br><br>
            Best viewed on a monitor and with a Chrome browser
        </h3>

        <br><br>
        <h2><b>TL; DR: Multi-view consistent depth estimation w.r.t the 3D scene where given a stream of images exist</b></h2>
    </center>

	<br><br>
	<hr>
	<br><br>

    <div class="docs-section" id="abstract">

		<center>
            <h1 class="navbar-heading"> <b>Abstract</b></h1>
        </center>

        <table align=center width=1350px>
            <tr>
                <td style="text-align:justify;">
					We are interested in achieving spatially accurate and temporally consistent depth estimates only from a stream of 2D RGB images.
					Despite the success of recent depth estimation methods, 
					we find that this is still difficult since existing approaches often estimate depth only from 2D information 
					and overlook how the scene exists in 3D space.
					To tackle the issue, we propose 
					<em>Multi-view Consistent depth estimation via Coordinated image-based neural rendering</em> 
					<!-- Multi-view Consistent depth estimation via Coordinated image-based neural rendering -->
					(MC<sup>2</sup>) which casts the depth estimation as a feature matching problem in 3D space, 
					thereby constructing and aligning scene features directly in 3D space from 2D images.
					First, we introduce a rescaling technique that minimizes the ambiguity of the depth estimation obtained independently from each 2D image.
					Using 2D images and corresponding rescaled depths, 
					we extract the context representation with our new transformer architecture consisting of three-way factorized attention.
					Moreover, to ensure alignment with 3D structures without explicit geometry modeling, 
					we propose an ordinal volume rendering that respects the nature of 3D spaces.
					We perform extensive comparisons on casually captured scenes from various real-world datasets 
					and significantly outperform previous work in depth estimation from a stream of 2D RGB images.
					Results highlight our method as a comprehensive framework 
					that not only improves the accuracy of monocular estimates 
					but also bridges the gap to multi-view consistent depth estimation 
					that respects the 3D worlds existing in given images.
                </td>
            </tr>
        </table>
    </div>


    </center>


	<br>
	<hr>

	<center>

		<!-- <center>
            <h1 class="navbar-heading"> <b>Overview of MC<sup>2</sup></b></h1>
        </center> -->

		<div class="video-comp-container">
			<img id="overview" src="resources/1600x900_tmp.gif" alt="Descriptive text here"  width="1600">
		</div>

		<table align=center width=1350px>
            <tr>
                <td style="text-align:justify;">
					MC<sup>2</sup> synthesizes a depth map and corresponding RGB image at <em>arbitrary camera angles</em> using a stream of 2D RGB images as context features. 
					Such an accurate depth prediction is enabled by three main components. 
					First, MC<sup>2</sup> rescales each depth estimate of the context views obtained from the monocular depth estimation network to make them exist in the 3D world 
					in which the scene exists and ensure consistency between individually obtained estimates, 
					thus enabling geometric priors during feature matching in the next phase. 
					Then, these rescaled depth features with image features are fed into our three-way factorized transformers 
					that decompose the features along view, ray, and pixel to efficiently find the correspondence between the context features.
					Finally, MC<sup>2</sup> renders a depth map without explicit proxy geometry by taking into account the sequential nature of a ray. 
					By doing so, MC<sup>2</sup> successfully imagines spatially accurate and temporally consistent depth maps that respect the 3D world in which the given images reside.
				
                </td>
            </tr>
        </table>


	</center>

	<br><br>
	<hr>
	<br><br>

    <br>
</body>

</html>